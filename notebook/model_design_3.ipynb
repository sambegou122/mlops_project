{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', delimiter=',')\n",
    "test = pd.read_csv('data/test.csv',  delimiter=',')\n",
    "valid = pd.read_csv('data/valid.csv',  delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Unnamed: 0', \"film-url\"], axis=1, inplace=True)\n",
    "test.drop(['Unnamed: 0', \"film-url\"], axis=1, inplace=True)\n",
    "valid.drop(['Unnamed: 0', \"film-url\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import sklearn\n",
    "\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment('Experiment Sentiment Analysis')\n",
    "mlflow.sklearn.autolog(log_datasets=False)\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    dataset,\n",
    "    dataset_test,\n",
    "    pipeline,\n",
    "    mlflow_run_tags = None,\n",
    "    mlflow_run_parameters = None,\n",
    "    mlflow_run_description = None,):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Build a sentiment analysis model, print the evaluation result and store everything to MLFlow\n",
    "    @param: dataset: pandas dataframe containing the input training set\n",
    "    @param: pipeline: scikit-learn pipeline that will be applied to the input data\n",
    "    @param: model_name: name of the model as it will be stored in MLFlow\n",
    "    @param: mlflow_run_tags: dict of tags that will be stored in the MLFlow run\n",
    "    @param: mlflow_run_parameters: dict of parameters that will be stored in the MLFlow run\n",
    "    @param: mlflow_run_description: textual description of the run\n",
    "    @param: mlflow_model_tags: dict of tags that will be stored in the MLFlow regietered model\n",
    "    @param: mlflow_model_description: textual description of the model    \n",
    "    @return: the ModelInfo of the model generated by MLFlow \n",
    "\n",
    "    \"\"\"\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        # Log parameters\n",
    "        if mlflow_run_parameters is not None:\n",
    "            for key, value in mlflow_run_parameters.items():\n",
    "                mlflow.log_param(key, value)\n",
    "        # Log tags\n",
    "        if mlflow_run_tags is not None:\n",
    "            for key, value in mlflow_run_tags.items():\n",
    "                mlflow.set_tag(key, value)\n",
    "        # Log description\n",
    "        if mlflow_run_description is not None:\n",
    "            mlflow.set_tag(\"description\", mlflow_run_description)\n",
    "        \n",
    "   \n",
    "      \n",
    "        X_train = dataset['review']\n",
    "  \n",
    "        y_train = dataset['polarity']\n",
    "        X_test = dataset_test['review']\n",
    "        y_test = dataset_test['polarity']\n",
    "\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        pred = pipeline.predict(X_test)\n",
    "\n",
    "        # signature = infer_signature(X_test, pred)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, pred)\n",
    "        f1 = f1_score(y_test, pred)\n",
    "\n",
    "        mlflow.log_metric('Test accuracy', accuracy)\n",
    "        mlflow.log_metric('Test f1  ', f1)\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sambegou/miniconda3/envs/mlops/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'qu', 'quelqu'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/sambegou/miniconda3/envs/mlops/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/sambegou/miniconda3/envs/mlops/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'qu', 'quelqu'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/sambegou/miniconda3/envs/mlops/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2922: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "stop = list(fr_stop)\n",
    "setp1 = [('vectorizer', TfidfVectorizer(stop_words=stop)), ('lr', LogisticRegression(penalty='l2', C=1.0))]\n",
    "step2 = [('vectorizer', TfidfVectorizer(stop_words=stop)), ('nb', MultinomialNB())]\n",
    "pipelines = [Pipeline(setp1), Pipeline(step2)]\n",
    "descriptions = [\"Logistic Regression with penalty\", \"Naive Bayes\"]\n",
    "dataset = train\n",
    "dataset_test = test\n",
    "\n",
    "for pipeline, description in zip(pipelines, descriptions):\n",
    "\n",
    "    build_model(dataset, dataset_test, pipeline, \\\n",
    "            mlflow_run_tags = None,\n",
    "            mlflow_run_parameters = None,\\\n",
    "        mlflow_run_description  = description)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
